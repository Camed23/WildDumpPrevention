"""
Classifier - Module de classification des poubelles pleines/vides

LOGIQUE G√âN√âRALE :
- S√©parer scoring (rules_engine) et classification (ce module)
- Utiliser des seuils configurables pour transformer les scores en pr√©dictions
- G√©rer l'incertitude avec une cat√©gorie "inconnu"
- Fournir des m√©triques de confiance pour √©valuer la qualit√© des pr√©dictions

ARCHITECTURE :
1. BinClassifier : Classe principale qui utilise RulesEngine + seuils
2. test_classifier() : Fonction pour √©valuer les performances sur un dataset
3. Gestion des erreurs et analyse pour am√©lioration continue
"""

# backend/services/classifier.py

import json
import backend.config as config
from backend.services.feature_extractor import ImageFeatures
from backend.services.rules_engine import RulesEngine
from collections import Counter

# Configuration
CACHE_PATH = "cache/images_metadata_labeled.json"
# CACHE_PATH = config.CACHE_PATH  # Utiliser la configuration une fois le projet termin√©


class BinClassifier:
    """
    Classificateur principal pour d√©terminer si une poubelle est pleine, vide ou incertaine
    
    LOGIQUE DE CONCEPTION :
    1. Utilise un RulesEngine pour obtenir un score [-1, +1]
    2. Applique des seuils pour convertir le score en classe
    3. Calcule une confiance bas√©e sur la distance aux seuils
    4. Permet l'ajustement dynamique des seuils
    
    AVANTAGES :
    - S√©paration claire entre scoring et classification
    - Gestion explicite de l'incertitude
    - M√©triques de confiance pour filtrer les pr√©dictions douteuses
    - Facilit√© d'ajustement des seuils selon le contexte d'usage
    """
    def __init__(self, rules_engine=None, thresholds=None):
        """
        Initialise le classificateur
        
        Args:
            rules_engine (RulesEngine): Moteur de r√®gles personnalis√© (optionnel)
            thresholds (dict): Seuils de classification personnalis√©s (optionnel)
            
        LOGIQUE DES SEUILS :
        - full_min : Score minimum pour √™tre s√ªr que c'est "plein"
        - empty_max : Score maximum pour √™tre s√ªr que c'est "vide"
        - Zone d'incertitude : entre empty_max et full_min
        - confidence_min : Seuil de confiance en dessous duquel on force "inconnu"
        """
        self.rules_engine = rules_engine or RulesEngine()
        self.thresholds = thresholds or {
            'full_min': 0.2,      # Score minimum pour "plein" (moins strict)
            'empty_max': -0.05,   # Score maximum pour "vide" (moins strict)
            'confidence_min': 0.1  # Confiance minimum (√©vite les pr√©dictions hasardeuses)
        }
        
    def classify(self, features):
        """
        Classifie une image en fonction de ses features extraites
        
        LOGIQUE DE CLASSIFICATION :
        1. Obtenir un score normalis√© [-1, +1] via le RulesEngine
        2. Appliquer les seuils pour d√©terminer la classe
        3. Calculer une confiance bas√©e sur la distance aux seuils
        4. Appliquer un filtre de confiance minimum
        
        INTERPR√âTATION DES SCORES :
        - score >= full_min (ex: 0.3) ‚Üí "plein" avec confiance = score
        - score <= empty_max (ex: -0.3) ‚Üí "vide" avec confiance = |score|
        - entre les deux ‚Üí "inconnu" (zone d'incertitude)
        - confiance < confidence_min ‚Üí forcer "inconnu" (pr√©diction pas fiable)
        
        Args:
            features (dict): Features extraites de l'image (area_ratio, contrast, etc.)
            
        Returns:
            dict: {
                'prediction': str,        # 'plein', 'vide', ou 'inconnu'
                'confidence': float,      # Confiance [0, 1] dans la pr√©diction
                'score': float,          # Score brut du RulesEngine [-1, +1]
                'details': dict          # D√©tails de l'√©valuation (r√®gles actives, etc.)
            }
        """
        # √âtape 1 : √âvaluation avec le moteur de r√®gles
        evaluation = self.rules_engine.evaluate(features)
        score = evaluation['score']  # Score normalis√© [-1, +1]
        
        # √âtape 2 : Classification bas√©e sur les seuils
        if score >= self.thresholds['full_min']:
            # Score √©lev√© = poubelle pleine
            prediction = "plein"
            confidence = min(score, 1.0)  # Confiance = proximit√© √† +1
        elif score <= self.thresholds['empty_max']:
            # Score faible = poubelle vide
            prediction = "vide"
            confidence = min(abs(score), 1.0)  # Confiance = proximit√© √† -1
        else:
            # Score dans la zone d'incertitude
            prediction = "inconnu"
            confidence = 1.0 - abs(score)  # Plus on est proche de 0, moins on est s√ªr
            
        # √âtape 3 : Filtre de confiance minimum
        # Si la confiance est trop faible, forcer "inconnu" pour √©viter les erreurs
        if confidence < self.thresholds['confidence_min']:
            prediction = "inconnu"
            
        return {
            'prediction': prediction,
            'confidence': confidence,
            'score': score,
            'details': evaluation
        }
    
    def set_thresholds(self, full_min=None, empty_max=None, confidence_min=None):
        """
        Ajuste les seuils de classification pour optimiser les performances
        
        UTILIT√â :
        - Adapter le comportement selon le contexte (tol√©rance aux erreurs)
        - Optimiser la pr√©cision apr√®s analyse des r√©sultats
        - Exp√©rimenter avec diff√©rents compromis pr√©cision/rappel
        
        Args:
            full_min (float): Seuil minimum pour "plein" (plus haut = plus strict)
            empty_max (float): Seuil maximum pour "vide" (plus bas = plus strict)  
            confidence_min (float): Confiance minimum requise (plus haut = plus d'"inconnu")
        """
        if full_min is not None:
            self.thresholds['full_min'] = full_min
        if empty_max is not None:
            self.thresholds['empty_max'] = empty_max
        if confidence_min is not None:
            self.thresholds['confidence_min'] = confidence_min


def test_classifier(cache_path=CACHE_PATH, show_details=False):
    """
    Fonction de test et d'√©valuation du classifier sur un dataset labellis√©
    
    OBJECTIFS :
    1. Mesurer la pr√©cision globale du syst√®me
    2. Analyser la r√©partition des pr√©dictions (plein/vide/inconnu)
    3. Identifier les erreurs pour am√©liorer les r√®gles
    4. Calculer des m√©triques de performance (confiance, scores)
    
    LOGIQUE D'ANALYSE :
    - Comparer pr√©dictions vs labels r√©els
    - Tracker les r√®gles impliqu√©es dans les erreurs
    - Calculer pr√©cision avec et sans les "inconnu"
    - Statistiques des scores pour validation du syst√®me
    
    Args:
        cache_path (str): Chemin vers le fichier JSON des images labellis√©es
        show_details (bool): Afficher les d√©tails des r√®gles pour debug
        
    Returns:
        tuple: (classifier, accuracy) pour usage programmatique
    """
    # Initialisation du classifier avec param√®tres par d√©faut
    classifier = BinClassifier()
    
    # Chargement du dataset labellis√© depuis le cache JSON
    with open(cache_path, "r", encoding="utf-8") as f:
        images = json.load(f)

    # Initialisation des m√©triques de performance
    total = len(images)                                          # Nombre total d'images
    correct = 0                                                  # Pr√©dictions correctes
    predictions_count = {"plein": 0, "vide": 0, "inconnu": 0}  # R√©partition des pr√©dictions
    scores = []                                                  # Scores pour analyse statistique
    confidences = []                                             # Niveaux de confiance
    errors_analysis = []                                         # Analyse d√©taill√©e des erreurs

    print("=== TEST DU CLASSIFIER MODERNE ===\n")
    
    # Boucle principale : tester chaque image du dataset
    for i, img in enumerate(images):
        # R√©cup√©ration du label attendu (v√©rit√© terrain)
        annotation = img.get("annotation", [{}])[0]
        true_label = annotation.get("label", "inconnu")

        # Extraction des features visuelles de l'image
        feats = ImageFeatures(img).extract_all_features()
        
        # Classification avec notre syst√®me
        result = classifier.classify(feats)
        predicted = result['prediction']    # Classe pr√©dite
        confidence = result['confidence']   # Confiance [0, 1]
        score = result['score']            # Score brut [-1, +1]
        
        # Mise √† jour des statistiques globales
        predictions_count[predicted] += 1
        scores.append(score)
        confidences.append(confidence)
        
        # Affichage du r√©sultat pour suivi en temps r√©el
        status = "‚úì" if predicted == true_label else "‚úó"  # Symbole visuel pour rapidit√©
        img_name = img.get('name_image', '')[:25]          # Nom tronqu√© pour lisibilit√©
        print(f"{status} {img_name:25} | {true_label:6} ‚Üí {predicted:6} | Score: {score:+.3f} | Conf: {confidence:.3f}")
        
        # √âvaluation de la pr√©diction et collecte des erreurs
        if predicted == true_label:
            correct += 1  # Compteur des pr√©dictions correctes
        else:
            # Analyse d√©taill√©e des erreurs pour am√©lioration future
            errors_analysis.append({
                'image': img_name,
                'expected': true_label,
                'predicted': predicted,
                'score': score,
                'confidence': confidence,
                'active_rules': result['details']['active_rules']  # R√®gles qui ont influenc√© l'erreur
            })
            
        # Debug d√©taill√© pour les premi√®res images (optionnel)
        if show_details and i < 3:
            print(f"    Rules actives: {result['details']['active_rules']}")
            print(f"    Score brut: {result['details']['raw_score']:.2f}")
    
    # === CALCUL ET AFFICHAGE DES M√âTRIQUES FINALES ===
    
    print(f"\n=== R√âSULTATS FINAUX ===")
    print(f"Pr√©cision globale: {correct}/{total} = {correct/total:.2%}")
    print(f"R√©partition: {predictions_count}")
    
    # Calcul de la pr√©cision en excluant les pr√©dictions "inconnu"
    # LOGIQUE : Les "inconnu" ne sont ni vrais ni faux, on s'int√©resse aux pr√©dictions fermes
    definitive = total - predictions_count["inconnu"]
    if definitive > 0:
        # Compter les pr√©dictions correctes parmi les pr√©dictions d√©finitives
        definitive_correct = sum(1 for err in errors_analysis if err['predicted'] != 'inconnu')
        definitive_correct = correct - (len(errors_analysis) - definitive_correct)
        definitive_accuracy = definitive_correct / definitive
        print(f"Pr√©cision sur pr√©dictions d√©finitives: {definitive_correct}/{definitive} = {definitive_accuracy:.2%}")
    
    # Statistiques des scores pour validation de la distribution
    if scores:
        print(f"\nStatistiques des scores:")
        print(f"  Moyenne: {sum(scores)/len(scores):+.3f}")        # Biais vers plein (+) ou vide (-)
        print(f"  Min: {min(scores):+.3f}, Max: {max(scores):+.3f}")  # √âtendue des scores
        print(f"  Confiance moyenne: {sum(confidences)/len(confidences):.3f}")  # Fiabilit√© g√©n√©rale
    
    # Analyse des erreurs pour identifier les am√©liorations possibles
    if errors_analysis:
        print(f"\nAnalyse des erreurs ({len(errors_analysis)} erreurs):")
        error_rules = []  # Collecte des r√®gles impliqu√©es dans les erreurs
        
        # Affichage des erreurs les plus repr√©sentatives
        for err in errors_analysis[:5]:  # Limiter √† 5 pour lisibilit√©
            print(f"  {err['image']:20} {err['expected']} ‚Üí {err['predicted']} (score: {err['score']:+.3f})")
            error_rules.extend(err['active_rules'])  # Accumuler les r√®gles probl√©matiques
        
        # Identification des r√®gles les plus souvent impliqu√©es dans les erreurs
        # UTILIT√â : Permet d'identifier les r√®gles √† revoir/ajuster
        if error_rules:
            rule_counter = Counter(error_rules)
            print(f"\nR√®gles les plus impliqu√©es dans les erreurs:")
            for rule, count in rule_counter.most_common(3):
                print(f"  {rule}: {count} fois")
                # CONSEIL : Examiner ces r√®gles pour ajuster seuils ou poids
    
    return classifier, correct/total


# === POINT D'ENTR√âE PRINCIPAL ===
if __name__ == "__main__":
    """
    Ex√©cution directe du module pour test rapide
    
    UTILISATION :
    - python backend/services/classifier.py
    - Ou depuis la racine : python -c "from backend.services.classifier import test_classifier; test_classifier()"
    
    CONSEIL : Utiliser show_details=True pour debug approfondi des r√®gles
    """
    print("üöÄ Test du classifier moderne...\n")
    classifier, accuracy = test_classifier(show_details=True)
    print(f"\nüéØ Accuracy finale: {accuracy:.2%}")
    
    # PROCHAINES √âTAPES sugg√©r√©es selon l'accuracy :
    if accuracy > 0.8:
        print("‚úÖ Bonnes performances ! Consid√©rer l'ajout de features avanc√©es.")
    elif accuracy > 0.6:
        print("‚ö†Ô∏è  Performances moyennes. Revoir les seuils et r√®gles probl√©matiques.")
    else:
        print("‚ùå Performances faibles. Revoir la logique des r√®gles et les features.")